{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28f300c7",
   "metadata": {},
   "source": [
    "## NOTE: \n",
    "it seems we won't be able to use chat templates, so no instruction fine tuning it seems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5146eb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from typing import Dict, List\n",
    "import argparse\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bc19b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-0.6B-Base\"  # Replace with your checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddb2e039",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"mps\",\n",
    "    use_cache=True\n",
    ").eval()\n",
    "\n",
    "# prepare the model input\n",
    "prompt = \"Once upon a time, there was a Wait,\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d273f4",
   "metadata": {},
   "source": [
    "Does Qwen do it like this?\n",
    "\n",
    "```\n",
    "<|im_start|>system\n",
    "You are a helpful assistant.<|im_end|>\n",
    "<|im_start|>user\n",
    "What is the capital of France?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "The capital of France is Paris.<|im_end|>\n",
    "```\n",
    "\n",
    "Instead of having <|system|> being special tokens?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a3d2053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- messages[0].content + \\'\\\\n\\\\n\\' }}\\n    {%- endif %}\\n    {{- \"# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0].content + \\'<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\\n{%- for message in messages[::-1] %}\\n    {%- set index = (messages|length - 1) - loop.index0 %}\\n    {%- if ns.multi_step_tool and message.role == \"user\" and not(message.content.startswith(\\'<tool_response>\\') and message.content.endswith(\\'</tool_response>\\')) %}\\n        {%- set ns.multi_step_tool = false %}\\n        {%- set ns.last_query_index = index %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {%- set content = message.content %}\\n        {%- set reasoning_content = \\'\\' %}\\n        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\\n            {%- set reasoning_content = message.reasoning_content %}\\n        {%- else %}\\n            {%- if \\'</think>\\' in message.content %}\\n                {%- set content = message.content.split(\\'</think>\\')[-1].lstrip(\\'\\\\n\\') %}\\n                {%- set reasoning_content = message.content.split(\\'</think>\\')[0].rstrip(\\'\\\\n\\').split(\\'<think>\\')[-1].lstrip(\\'\\\\n\\') %}\\n            {%- endif %}\\n        {%- endif %}\\n        {%- if loop.index0 > ns.last_query_index %}\\n            {%- if loop.last or (not loop.last and reasoning_content) %}\\n                {{- \\'<|im_start|>\\' + message.role + \\'\\\\n<think>\\\\n\\' + reasoning_content.strip(\\'\\\\n\\') + \\'\\\\n</think>\\\\n\\\\n\\' + content.lstrip(\\'\\\\n\\') }}\\n            {%- else %}\\n                {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + content }}\\n            {%- endif %}\\n        {%- else %}\\n            {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + content }}\\n        {%- endif %}\\n        {%- if message.tool_calls %}\\n            {%- for tool_call in message.tool_calls %}\\n                {%- if (loop.first and content) or (not loop.first) %}\\n                    {{- \\'\\\\n\\' }}\\n                {%- endif %}\\n                {%- if tool_call.function %}\\n                    {%- set tool_call = tool_call.function %}\\n                {%- endif %}\\n                {{- \\'<tool_call>\\\\n{\"name\": \"\\' }}\\n                {{- tool_call.name }}\\n                {{- \\'\", \"arguments\": \\' }}\\n                {%- if tool_call.arguments is string %}\\n                    {{- tool_call.arguments }}\\n                {%- else %}\\n                    {{- tool_call.arguments | tojson }}\\n                {%- endif %}\\n                {{- \\'}\\\\n</tool_call>\\' }}\\n            {%- endfor %}\\n        {%- endif %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n    {%- if enable_thinking is defined and enable_thinking is false %}\\n        {{- \\'<think>\\\\n\\\\n</think>\\\\n\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f836c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eos_token': '<|im_end|>',\n",
       " 'pad_token': '<|endoftext|>',\n",
       " 'additional_special_tokens': ['<|im_start|>',\n",
       "  '<|im_end|>',\n",
       "  '<|object_ref_start|>',\n",
       "  '<|object_ref_end|>',\n",
       "  '<|box_start|>',\n",
       "  '<|box_end|>',\n",
       "  '<|quad_start|>',\n",
       "  '<|quad_end|>',\n",
       "  '<|vision_start|>',\n",
       "  '<|vision_end|>',\n",
       "  '<|vision_pad|>',\n",
       "  '<|image_pad|>',\n",
       "  '<|video_pad|>']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "969b62fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8e6c5a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eos_token': '<|im_end|>',\n",
       " 'pad_token': '<|endoftext|>',\n",
       " 'additional_special_tokens': ['<|im_start|>',\n",
       "  '<|im_end|>',\n",
       "  '<|object_ref_start|>',\n",
       "  '<|object_ref_end|>',\n",
       "  '<|box_start|>',\n",
       "  '<|box_end|>',\n",
       "  '<|quad_start|>',\n",
       "  '<|quad_end|>',\n",
       "  '<|vision_start|>',\n",
       "  '<|vision_end|>',\n",
       "  '<|vision_pad|>',\n",
       "  '<|image_pad|>',\n",
       "  '<|video_pad|>']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "031c80c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eos_token': AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 'pad_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 'additional_special_tokens': ['<|im_start|>',\n",
       "  '<|im_end|>',\n",
       "  '<|object_ref_start|>',\n",
       "  '<|object_ref_end|>',\n",
       "  '<|box_start|>',\n",
       "  '<|box_end|>',\n",
       "  '<|quad_start|>',\n",
       "  '<|quad_end|>',\n",
       "  '<|vision_start|>',\n",
       "  '<|vision_end|>',\n",
       "  '<|vision_pad|>',\n",
       "  '<|image_pad|>',\n",
       "  '<|video_pad|>']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea4f32e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 'pad_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 'additional_special_tokens': ['<unk>', '<s>', '</s>']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/mistral-7b-sft-beta\")\n",
    "\n",
    "tokenizer.special_tokens_map_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79ece06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids('<s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55c17d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing '<|assistant|>' results in: ['▁<', '|', 'ass', 'istant', '|', '>']\n",
      "<|assistant|> is NOT tokenized as a single special token.\n"
     ]
    }
   ],
   "source": [
    "tokens_assistant = tokenizer.tokenize('<|assistant|>')\n",
    "print(f\"Tokenizing '<|assistant|>' results in: {tokens_assistant}\")\n",
    "if len(tokens_assistant) == 1 and tokens_assistant[0] == '<|assistant|>':\n",
    "    print(\"<|assistant|> is tokenized as a single special token.\")\n",
    "else:\n",
    "    print(\"<|assistant|> is NOT tokenized as a single special token.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c54e9f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[523,\n",
       " 28766,\n",
       " 1838,\n",
       " 28766,\n",
       " 28767,\n",
       " 13,\n",
       " 18325,\n",
       " 547,\n",
       " 264,\n",
       " 10537,\n",
       " 5643,\n",
       " 1101,\n",
       " 2,\n",
       " 28705,\n",
       " 13,\n",
       " 28789,\n",
       " 28766,\n",
       " 489,\n",
       " 11143,\n",
       " 28766,\n",
       " 28767,\n",
       " 13,\n",
       " 28792,\n",
       " 28735,\n",
       " 2729,\n",
       " 19479,\n",
       " 628,\n",
       " 15548,\n",
       " 28793,\n",
       " 3850,\n",
       " 2,\n",
       " 28705,\n",
       " 13]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = {\n",
    "  \"messages\": [\n",
    "    {\"role\": \"user\", \"content\": \"Provide a detailed analysis...\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"[Sports Commentary Script] ...\"}\n",
    "  ]\n",
    "}\n",
    "\n",
    "tokenized = tokenizer.apply_chat_template(example[\"messages\"], tokenize=True, truncation=True, max_length=2048)\n",
    "\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e190ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36fd1a93",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      2\u001b[39m     tokens = tokenizer.apply_chat_template(\n\u001b[32m      3\u001b[39m         example[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m], tokenize=\u001b[38;5;28;01mTrue\u001b[39;00m, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, max_length=\u001b[32m2048\u001b[39m\n\u001b[32m      4\u001b[39m     )\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m: tokens}\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m tokenized_dataset = \u001b[43mdataset\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m].map(preprocess, remove_columns=[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "def preprocess(example):\n",
    "    tokens = tokenizer.apply_chat_template(\n",
    "        example[\"messages\"], tokenize=True, truncation=True, max_length=2048\n",
    "    )\n",
    "    return {\"input_ids\": tokens}\n",
    "\n",
    "tokenized_dataset = dataset[\"train\"].map(preprocess, remove_columns=[\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "910ad0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9faf63ce4854209a2f0300926a78092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd15819f6384bd289ef986bf47eab00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3432fd506a3a47ea8c15d8a6fd018eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e2890ebfaa4258a95bc9e35c109a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available splits: ['python', 'cpp']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6d09b4d8834df19f569d98736ade2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b48bdec785754e7690cb4c7c5ba819bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config names:  ['train']\n"
     ]
    }
   ],
   "source": [
    "from datasets import get_dataset_config_names, get_dataset_split_names, load_dataset\n",
    "\n",
    "splits = get_dataset_split_names(\"nvidia/OpenCodeReasoning-2\")\n",
    "print(\"Available splits:\", splits)\n",
    "\n",
    "print(\"config names: \", get_dataset_config_names(\"nvidia/OpenCodeReasoning-2\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6aa209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5571424ef0403faf62070ab978672f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f291787401314e68ab03c7995b299a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e48e3091d504ac0bd01757ac887146a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86227814b2a6466e8d380a8cc672aea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61e165ce2c0458a8db58923a61a484d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/70 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa29430008984aef800e5bcb31aa16df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00001-of-00070.parquet:  86%|########5 | 315M/367M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7b5897fafb451ab80c3830ffc75bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00002-of-00070.parquet:   0%|          | 0.00/365M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"nvidia/OpenCodeReasoning-2\", name=\"train\", split=\"train[:1%]\") # to load just a part of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bd74be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load just 1% of the dataset by using the streaming mode and sampling\n",
    "dataset = load_dataset(\n",
    "    \"your_dataset_name\",  # Replace with actual dataset name\n",
    "    split=\"train\",  # Or whichever split you want\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "# Sample just 1% using the streaming API\n",
    "sampled_dataset = dataset.shuffle(seed=42).take_percent(1)\n",
    "\n",
    "# Convert the streaming dataset to a regular dataset in memory\n",
    "# This downloads only the sampled 1%\n",
    "sampled_dataset = sampled_dataset.map(lambda x: x, batched=False)\n",
    "\n",
    "sampled_dataset.save_to_disk(\"my_1percent_dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcqa_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
