defaults:
  - override hydra/job_logging: disabled

environment:
  seed: 42

model:
  name: Qwen/Qwen3-0.6B-Base
  hub_model_id: andresnowak/Qwen3-0.6B-instruction-finetuned

dataset:
  - name: andresnowak/Instruction-finetuning-mixture-mnlp
    config: all
    samples: 200000
  - name: cais/mmlu
    config: default
    samples: 20000

training:
  output_dir: ./output
  logging_dir: ./logs
  resume_dir: None
  report_to: wandb
  learning_rate: 1e-5  # Default value instead of 5e-6
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 16 # to get effective 128
  num_train_epochs: 2
  weight_decay: 0.00
  warmup_ratio: 0.3
  max_grad_norm: 1.0
  linear_layers_max_grad_norm: 1.0

wandb:
  project: MNLP-qwen-instruction-finetuning
  name: qwen-instruction-finetuning


# from datasets import interleave_datasets

# mixture = interleave_datasets(
#     [ds1, ds2, ds3],
#     probabilities=[1.0, 1.0, 1.0]
# )
